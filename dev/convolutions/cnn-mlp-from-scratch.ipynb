{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root='/mnt/d/CUDA/cuda-learn/mnist-cuda/data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "X_train = mnist_train.data.numpy().reshape(-1, 1, 28, 28) / 255.0\n",
    "y_train = mnist_train.targets.numpy()\n",
    "X_test = mnist_test.data.numpy().reshape(-1, 1, 28, 28) / 255.0\n",
    "y_test = mnist_test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.w = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2. / (in_channels * kernel_size * kernel_size))\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = np.pad(x, ((0,0), (0,0), (self.padding,self.padding), (self.padding,self.padding)), mode='constant')\n",
    "        n, c, h, w = self.x.shape\n",
    "        out_h = (h - self.w.shape[2]) // self.stride + 1\n",
    "        out_w = (w - self.w.shape[3]) // self.stride + 1\n",
    "        out = np.zeros((n, self.w.shape[0], out_h, out_w))\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                out[:, :, i, j] = np.sum(self.x[:, np.newaxis, :, i*self.stride:i*self.stride+self.w.shape[2], j*self.stride:j*self.stride+self.w.shape[3]] * self.w[np.newaxis, :, :, :, :], axis=(2,3,4))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        n, _, out_h, out_w = dout.shape\n",
    "        dx = np.zeros_like(self.x)\n",
    "        dw = np.zeros_like(self.w)\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                x_slice = self.x[:, :, i*self.stride:i*self.stride+self.w.shape[2], j*self.stride:j*self.stride+self.w.shape[3]]\n",
    "                for k in range(self.w.shape[0]):  # out_channels\n",
    "                    dx[:, :, i*self.stride:i*self.stride+self.w.shape[2], j*self.stride:j*self.stride+self.w.shape[3]] += self.w[k, :, :, :] * dout[:, k, i, j][:, np.newaxis, np.newaxis, np.newaxis]\n",
    "                    dw[k, :, :, :] += np.sum(x_slice * dout[:, k, i, j][:, np.newaxis, np.newaxis, np.newaxis], axis=0)\n",
    "\n",
    "        return dx[:, :, self.padding:-self.padding, self.padding:-self.padding], dw\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * (self.x > 0)\n",
    "\n",
    "class MaxPool:\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        n, c, h, w = x.shape\n",
    "        out_h = (h - self.kernel_size) // self.stride + 1\n",
    "        out_w = (w - self.kernel_size) // self.stride + 1\n",
    "        out = np.zeros((n, c, out_h, out_w))\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                out[:, :, i, j] = np.max(x[:, :, i*self.stride:i*self.stride+self.kernel_size, j*self.stride:j*self.stride+self.kernel_size], axis=(2,3))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        n, c, out_h, out_w = dout.shape\n",
    "        dx = np.zeros_like(self.x)\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                window = self.x[:, :, i*self.stride:i*self.stride+self.kernel_size, j*self.stride:j*self.stride+self.kernel_size]\n",
    "                mask = window == np.max(window, axis=(2,3))[:, :, np.newaxis, np.newaxis]\n",
    "                dx[:, :, i*self.stride:i*self.stride+self.kernel_size, j*self.stride:j*self.stride+self.kernel_size] += mask * dout[:, :, i:i+1, j:j+1]\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.w = np.random.randn(out_features, in_features) * np.sqrt(2. / in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.w @ x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.w.T @ dout\n",
    "        dw = dout @ self.x.T\n",
    "        return dx, dw\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_pred.shape[1]\n",
    "    p = softmax(y_pred)\n",
    "    log_likelihood = -np.log(p[y_true, range(m)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        self.conv1 = ConvLayer(1, 32, 3, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool(2, 2)\n",
    "        self.fc1 = Linear(32 * 14 * 14, 128)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc2 = Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        x = x.reshape(x.shape[0], -1).T\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx, fc2_grad = self.fc2.backward(dout)\n",
    "        dx = self.relu2.backward(dx)\n",
    "        dx, fc1_grad = self.fc1.backward(dx)\n",
    "        dx = dx.T.reshape(dx.shape[1], 32, 14, 14)\n",
    "        dx = self.pool1.backward(dx)\n",
    "        dx = self.relu1.backward(dx)\n",
    "        dx, conv_grad = self.conv1.backward(dx)\n",
    "\n",
    "        return conv_grad, fc1_grad, fc2_grad\n",
    "\n",
    "    def update_weights(self, conv_grad, fc1_grad, fc2_grad, lr):\n",
    "        self.conv1.w -= lr * conv_grad\n",
    "        self.fc1.w -= lr * fc1_grad\n",
    "        self.fc2.w -= lr * fc2_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Iter: 0 Loss: 2.530645441043551\n",
      "Iter: 8 Loss: 2.5112406822256244\n",
      "Iter: 16 Loss: 2.0873742338570302\n",
      "Iter: 24 Loss: 2.0589058731563217\n",
      "Iter: 32 Loss: 2.2173625868941906\n",
      "Iter: 40 Loss: 1.9335373665680442\n",
      "Iter: 48 Loss: 1.9149129443642696\n",
      "Iter: 56 Loss: 1.7693827838061982\n",
      "Iter: 64 Loss: 1.9623951144675247\n",
      "Iter: 72 Loss: 1.8624138235382963\n",
      "Iter: 80 Loss: 1.8460923005245462\n",
      "Iter: 88 Loss: 1.600983481219525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m dout \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_y)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m conv, fc1_grad, fc2_grad \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate_weights(conv, fc1_grad, fc2_grad, lr)\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mNeuralNet.backward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     25\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1\u001b[38;5;241m.\u001b[39mbackward(dx)\n\u001b[1;32m     26\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mbackward(dx)\n\u001b[0;32m---> 27\u001b[0m dx, conv_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv_grad, fc1_grad, fc2_grad\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mConvLayer.backward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):  \u001b[38;5;66;03m# out_channels\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             dx[:, :, i\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], j\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride:j\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw[k, :, :, :] \u001b[38;5;241m*\u001b[39m dout[:, k, i, j][:, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m---> 30\u001b[0m             dw[k, :, :, :] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dx[:, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding:\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding:\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding], dw\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:2250\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \n\u001b[1;32m   2246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2251\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2257\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "lr = 1e-3  # Further reduced learning rate\n",
    "\n",
    "model = NeuralNet()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model.forward(batch_X)\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        loss = cross_entropy_loss(y_pred, batch_y)\n",
    "        dout = softmax(y_pred)\n",
    "        dout[batch_y, range(len(batch_y))] -= 1\n",
    "        dout /= len(batch_y)\n",
    " \n",
    "        # Backward pass\n",
    "        conv, fc1_grad, fc2_grad = model.backward(dout)\n",
    "\n",
    "        # Update weights\n",
    "        model.update_weights(conv, fc1_grad, fc2_grad, lr)\n",
    "\n",
    "        if i % 64 == 0:\n",
    "            print(f\"Iter: {i//batch_size} Loss: {loss}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = model.forward(X_test)\n",
    "    test_loss = cross_entropy_loss(y_pred, y_test)\n",
    "    accuracy = np.mean(np.argmax(y_pred, axis=0) == y_test)\n",
    "    print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
