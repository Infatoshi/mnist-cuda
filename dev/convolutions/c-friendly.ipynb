{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root='/mnt/d/CUDA/cuda-learn/mnist-cuda/data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "X_train = mnist_train.data.numpy().reshape(-1, 1, 28, 28) / 255.0\n",
    "y_train = mnist_train.targets.numpy()\n",
    "X_test = mnist_test.data.numpy().reshape(-1, 1, 28, 28) / 255.0\n",
    "y_test = mnist_test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convolutional Layer\n",
    "def conv2d_init(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2. / (in_channels * kernel_size * kernel_size))\n",
    "\n",
    "def conv2d_forward(x, w, stride, padding):\n",
    "    x_padded = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)), mode='constant')\n",
    "    n, c, h, w_dim = x_padded.shape\n",
    "    out_h = (h - w.shape[2]) // stride + 1\n",
    "    out_w = (w_dim - w.shape[3]) // stride + 1\n",
    "    out = np.zeros((n, w.shape[0], out_h, out_w))\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            out[:, :, i, j] = np.sum(x_padded[:, np.newaxis, :, i*stride:i*stride+w.shape[2], j*stride:j*stride+w.shape[3]] * w[np.newaxis, :, :, :, :], axis=(2,3,4))\n",
    "\n",
    "    return out, x_padded\n",
    "\n",
    "def conv2d_backward(dout, x_padded, w, stride, padding):\n",
    "    n, _, out_h, out_w = dout.shape\n",
    "    dx = np.zeros_like(x_padded)\n",
    "    dw = np.zeros_like(w)\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            x_slice = x_padded[:, :, i*stride:i*stride+w.shape[2], j*stride:j*stride+w.shape[3]]\n",
    "            for k in range(w.shape[0]):  # out_channels\n",
    "                dx[:, :, i*stride:i*stride+w.shape[2], j*stride:j*stride+w.shape[3]] += w[k, :, :, :] * dout[:, k, i, j][:, np.newaxis, np.newaxis, np.newaxis]\n",
    "                dw[k, :, :, :] += np.sum(x_slice * dout[:, k, i, j][:, np.newaxis, np.newaxis, np.newaxis], axis=0)\n",
    "\n",
    "    return dx[:, :, padding:-padding, padding:-padding], dw\n",
    "\n",
    "# ReLU Activation\n",
    "def relu_forward(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_backward(dout, x):\n",
    "    return dout * (x > 0)\n",
    "\n",
    "# Max Pooling\n",
    "def maxpool_forward(x, kernel_size, stride):\n",
    "    n, c, h, w = x.shape\n",
    "    out_h = (h - kernel_size) // stride + 1\n",
    "    out_w = (w - kernel_size) // stride + 1\n",
    "    out = np.zeros((n, c, out_h, out_w))\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            out[:, :, i, j] = np.max(x[:, :, i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size], axis=(2,3))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maxpool_backward(dout, x, kernel_size, stride):\n",
    "    n, c, out_h, out_w = dout.shape\n",
    "    dx = np.zeros_like(x)\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            window = x[:, :, i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size]\n",
    "            mask = window == np.max(window, axis=(2,3))[:, :, np.newaxis, np.newaxis]\n",
    "            dx[:, :, i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size] += mask * dout[:, :, i:i+1, j:j+1]\n",
    "\n",
    "    return dx\n",
    "\n",
    "# Linear Layer\n",
    "def linear_init(in_features, out_features):\n",
    "    return np.random.randn(out_features, in_features) * np.sqrt(2. / in_features)\n",
    "\n",
    "def linear_forward(x, w):\n",
    "    return w @ x\n",
    "\n",
    "def linear_backward(dout, x, w):\n",
    "    dx = w.T @ dout\n",
    "    dw = dout @ x.T\n",
    "    return dx, dw\n",
    "\n",
    "# Softmax and Cross-Entropy Loss\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_pred.shape[1]\n",
    "    p = softmax(y_pred)\n",
    "    log_likelihood = -np.log(p[y_true, range(m)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "# Neural Network\n",
    "def init_neural_net():\n",
    "    return {\n",
    "        'conv1': conv2d_init(1, 32, 3, padding=1),\n",
    "        'fc1': linear_init(32 * 14 * 14, 128),\n",
    "        'fc2': linear_init(128, 10)\n",
    "    }\n",
    "\n",
    "def forward(x, weights):\n",
    "    conv1, x_padded = conv2d_forward(x, weights['conv1'], stride=1, padding=1)\n",
    "    relu1 = relu_forward(conv1)\n",
    "    pool1 = maxpool_forward(relu1, kernel_size=2, stride=2)\n",
    "    fc1_input = pool1.reshape(pool1.shape[0], -1).T\n",
    "    fc1 = linear_forward(fc1_input, weights['fc1'])\n",
    "    relu2 = relu_forward(fc1)\n",
    "    fc2 = linear_forward(relu2, weights['fc2'])\n",
    "    return fc2, (x_padded, conv1, relu1, pool1, fc1_input, fc1, relu2)\n",
    "\n",
    "def backward(dout, weights, cache):\n",
    "    x_padded, conv1, relu1, pool1, fc1_input, fc1, relu2 = cache\n",
    "    \n",
    "    dx, fc2_grad = linear_backward(dout, relu2, weights['fc2'])\n",
    "    dx = relu_backward(dx, fc1)\n",
    "    dx, fc1_grad = linear_backward(dx, fc1_input, weights['fc1'])\n",
    "    dx = dx.T.reshape(8, 32, 14, 14)\n",
    "    dx = maxpool_backward(dx, relu1, kernel_size=2, stride=2)\n",
    "    dx = relu_backward(dx, conv1)\n",
    "    dx, conv_grad = conv2d_backward(dx, x_padded, weights['conv1'], stride=1, padding=1)\n",
    "\n",
    "    return conv_grad, fc1_grad, fc2_grad\n",
    "\n",
    "def update_weights(weights, conv_grad, fc1_grad, fc2_grad, lr):\n",
    "    weights['conv1'] -= lr * conv_grad\n",
    "    weights['fc1'] -= lr * fc1_grad\n",
    "    weights['fc2'] -= lr * fc2_grad\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Iter: 0 Loss: 2.2550391329952015\n",
      "Iter: 8 Loss: 2.2528827395562807\n",
      "Iter: 16 Loss: 2.1885430698947284\n",
      "Iter: 24 Loss: 2.180439342893292\n",
      "Iter: 32 Loss: 2.161353204361432\n",
      "Iter: 40 Loss: 1.8303605009516488\n",
      "Iter: 48 Loss: 2.089944378459229\n",
      "Iter: 56 Loss: 1.7376544584815936\n",
      "Iter: 64 Loss: 2.167860412211517\n",
      "Iter: 72 Loss: 1.8911323782390927\n",
      "Iter: 80 Loss: 1.9841146376951972\n",
      "Iter: 88 Loss: 1.8443487545389812\n",
      "Iter: 96 Loss: 1.8257833455251125\n",
      "Iter: 104 Loss: 1.667480792769398\n",
      "Iter: 112 Loss: 1.847660729116419\n",
      "Iter: 120 Loss: 1.6279789866556955\n",
      "Iter: 128 Loss: 1.7849004907062522\n",
      "Iter: 136 Loss: 1.3919290212004989\n",
      "Iter: 144 Loss: 1.1411015059230274\n",
      "Iter: 152 Loss: 1.9223132772909188\n",
      "Iter: 160 Loss: 1.716764176848298\n",
      "Iter: 168 Loss: 1.6172682701563927\n",
      "Iter: 176 Loss: 1.5977699217614196\n",
      "Iter: 184 Loss: 1.6243539041111101\n",
      "Iter: 192 Loss: 1.4271120187400197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m dout \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_y)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m conv_grad, fc1_grad, fc2_grad \u001b[38;5;241m=\u001b[39m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     42\u001b[0m weights \u001b[38;5;241m=\u001b[39m update_weights(weights, conv_grad, fc1_grad, fc2_grad, lr)\n",
      "Cell \u001b[0;32mIn[40], line 117\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(dout, weights, cache)\u001b[0m\n\u001b[1;32m    115\u001b[0m dx \u001b[38;5;241m=\u001b[39m maxpool_backward(dx, relu1, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    116\u001b[0m dx \u001b[38;5;241m=\u001b[39m relu_backward(dx, conv1)\n\u001b[0;32m--> 117\u001b[0m dx, conv_grad \u001b[38;5;241m=\u001b[39m \u001b[43mconv2d_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv_grad, fc1_grad, fc2_grad\n",
      "Cell \u001b[0;32mIn[40], line 30\u001b[0m, in \u001b[0;36mconv2d_backward\u001b[0;34m(dout, x_padded, w, stride, padding)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):  \u001b[38;5;66;03m# out_channels\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             dx[:, :, i\u001b[38;5;241m*\u001b[39mstride:i\u001b[38;5;241m*\u001b[39mstride\u001b[38;5;241m+\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], j\u001b[38;5;241m*\u001b[39mstride:j\u001b[38;5;241m*\u001b[39mstride\u001b[38;5;241m+\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m w[k, :, :, :] \u001b[38;5;241m*\u001b[39m dout[:, k, i, j][:, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m---> 30\u001b[0m             dw[k, :, :, :] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dx[:, :, padding:\u001b[38;5;241m-\u001b[39mpadding, padding:\u001b[38;5;241m-\u001b[39mpadding], dw\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:2250\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \n\u001b[1;32m   2246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2251\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2257\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "lr = 1e-3  # Further reduced learning rate\n",
    "\n",
    "# Initialize the neural network\n",
    "weights = init_neural_net()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred, cache = forward(batch_X, weights)\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        loss = cross_entropy_loss(y_pred, batch_y)\n",
    "\n",
    "        # Compute softmax probabilities\n",
    "        softmax_probs = softmax(y_pred)\n",
    "\n",
    "        # Create one-hot encoded true labels\n",
    "        y_true = np.zeros_like(y_pred)\n",
    "        for k in range(len(batch_y)):\n",
    "            true_class = batch_y[k]\n",
    "            y_true[true_class, k] = 1\n",
    "\n",
    "        # print('softmax probs', softmax_probs, 'y_true', y_true)\n",
    "        # Compute gradient\n",
    "        dout = softmax_probs - y_true\n",
    "\n",
    "        # Normalize gradient by batch size\n",
    "        dout /= len(batch_y)\n",
    " \n",
    "        # Backward pass\n",
    "        conv_grad, fc1_grad, fc2_grad = backward(dout, weights, cache)\n",
    "\n",
    "        # Update weights\n",
    "        weights = update_weights(weights, conv_grad, fc1_grad, fc2_grad, lr)\n",
    "\n",
    "        if i % 64 == 0:\n",
    "            print(f\"Iter: {i//batch_size} Loss: {loss}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred, _ = forward(X_test, weights)\n",
    "    test_loss = cross_entropy_loss(y_pred, y_test)\n",
    "    accuracy = np.mean(np.argmax(y_pred, axis=0) == y_test)\n",
    "    print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
