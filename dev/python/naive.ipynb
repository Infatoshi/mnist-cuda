{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Define necessary naive operations\n",
    "def conv2d_naive(x, weights, stride=1, padding=1):\n",
    "    # Apply padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant', constant_values=0)\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = weights.shape\n",
    "    H_out = int(1 + (H + 2 * padding - HH) / stride)\n",
    "    W_out = int(1 + (W + 2 * padding - WW) / stride)\n",
    "    out = np.zeros((N, F, H_out, W_out))\n",
    "\n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + HH\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + WW\n",
    "            x_patched = x_padded[:, :, h_start:h_end, w_start:w_end]\n",
    "            for k in range(F):\n",
    "                out[:, k, i, j] = np.sum(x_patched * weights[k, :, :, :], axis=(1, 2, 3))\n",
    "\n",
    "    return out\n",
    "\n",
    "def conv2d_backward_naive(dout, x, w, stride=1, padding=1):\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    H_out = int(1 + (H + 2 * padding - HH) / stride)\n",
    "    W_out = int(1 + (W + 2 * padding - WW) / stride)\n",
    "    \n",
    "    # Using padded version of input\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant', constant_values=0)\n",
    "    dx_padded = np.zeros_like(x_padded)\n",
    "    dw = np.zeros_like(w)\n",
    "    \n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + HH\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + WW\n",
    "\n",
    "            for n in range(N):\n",
    "                idout = dout[n, :, i, j][:, None, None, None]\n",
    "                dx_padded[n, :, h_start:h_end, w_start:w_end] += np.sum(w * idout, axis=0)\n",
    "                dw += x_padded[n, :, h_start:h_end, w_start:w_end] * idout\n",
    "            \n",
    "    dx = dx_padded[:, :, padding:-padding, padding:-padding] if padding > 0 else dx_padded    \n",
    "    \n",
    "    return dx, dw\n",
    "\n",
    "# def conv2d_backward_naive(X, dL_dY, W_shape=(batch_size, 32), stride=1, padding=1):\n",
    "#     # Extract dimensions\n",
    "#     n_filters, d_filter, h_filter, w_filter = W_shape\n",
    "#     n_x, d_x, h_x, w_x = X.shape\n",
    "#     n_y, d_y, h_y, w_y = dL_dY.shape\n",
    "    \n",
    "#     # Initialize the gradient for the filter weights\n",
    "#     dL_dW = np.zeros(W_shape)\n",
    "    \n",
    "#     # Consider padding\n",
    "#     X_p = np.pad(X, [(0, 0), (0, 0), (padding, padding), (padding, padding)], mode='constant')\n",
    "    \n",
    "#     # Compute the gradient\n",
    "#     for i in range(h_y):\n",
    "#         for j in range(w_y):\n",
    "#             for f in range(n_filters):\n",
    "#                 h_start = i * stride\n",
    "#                 h_end = h_start + h_filter\n",
    "#                 w_start = j * stride\n",
    "#                 w_end = w_start + w_filter\n",
    "#                 dL_dW[f, :, :, :] += dL_dY[:, f, i, j][:, None, None, None] * X_p[:, :, h_start:h_end, w_start:w_end]\n",
    "    \n",
    "#     return dL_dW\n",
    "\n",
    "# Update Naive BatchNorm to return cache\n",
    "def batch_norm_naive(x, gamma, beta, eps=1e-5):\n",
    "    N, C, H, W = x.shape\n",
    "    mean = np.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "    var = np.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "    x_norm = (x - mean) / np.sqrt(var + eps)\n",
    "    out = gamma * x_norm + beta\n",
    "    cache = (x, x_norm, mean, var, gamma, beta, eps) # this one\n",
    "    return out, cache\n",
    "\n",
    "def d_batch_norm_naive(x, gamma, beta, dout, mean, var, eps=1e-5):\n",
    "    N, C, H, W = x.shape\n",
    "    x_norm = (x - mean) / np.sqrt(var + eps)\n",
    "    \n",
    "    dgamma = np.sum(dout * x_norm, axis=(0, 2, 3), keepdims=True)\n",
    "    dbeta = np.sum(dout, axis=(0, 2, 3), keepdims=True)\n",
    "    \n",
    "    dx_norm = dout * gamma\n",
    "    dvar = np.sum(dx_norm * (x - mean) * -0.5 * (var + eps)**(-1.5), axis=(0, 2, 3), keepdims=True)\n",
    "    dmean = np.sum(dx_norm * -1.0 / np.sqrt(var + eps), axis=(0, 2, 3), keepdims=True) + dvar * np.sum(-2.0 * (x - mean), axis=(0, 2, 3), keepdims=True) / N\n",
    "    dx = dx_norm * 1.0 / np.sqrt(var + eps) + dvar * 2.0 * (x - mean) / N + dmean / N\n",
    "    \n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "def relu_naive(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_relu_naive(x):\n",
    "    return (x > 0).astype(x.dtype)\n",
    "\n",
    "def max_pool2d_naive(x, pool_size=2, stride=2):\n",
    "    N, C, H, W = x.shape\n",
    "    H_out = int((H - pool_size) / stride + 1)\n",
    "    W_out = int((W - pool_size) / stride + 1)\n",
    "    out = np.zeros((N, C, H_out, W_out))\n",
    "\n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + pool_size\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + pool_size\n",
    "            x_patched = x[:, :, h_start:h_end, w_start:w_end]\n",
    "            out[:, :, i, j] = np.max(x_patched, axis=(2, 3))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def max_pool2d_backward_naive(prev_layer_grad, x, pool_size=2, stride=2):\n",
    "    \"\"\"\n",
    "    Backward pass for a 2D max-pooling layer.\n",
    "    \n",
    "    Args:\n",
    "    - prev_layer_grad: Gradient of the loss with respect to the outputs of the max pooling layer (shape: (N, C, H_out, W_out)).\n",
    "    - x: Input to the max pooling layer during the forward pass (shape: (N, C, H_in, W_in)).\n",
    "    - pool_size: Size of the pooling window (default 2).\n",
    "    - stride: Stride of the pooling window (default 2).\n",
    "    \n",
    "    Returns:\n",
    "    - dx: Gradient of the loss with respect to the inputs of the max pooling layer (shape: (N, C, H_in, W_in)).\n",
    "    \"\"\"\n",
    "    N, C, H_in, W_in = x.shape\n",
    "    H_out = (H_in - pool_size) // stride + 1\n",
    "    W_out = (W_in - pool_size) // stride + 1\n",
    "    \n",
    "    # Initialize the gradient with respect to input with zeros\n",
    "    dx = np.zeros_like(x)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    h_start = h * stride\n",
    "                    h_end = h_start + pool_size\n",
    "                    w_start = w * stride\n",
    "                    w_end = w_start + pool_size\n",
    "\n",
    "                    # Extract the current pooling region from the input\n",
    "                    x_pool = x[n, c, h_start:h_end, w_start:w_end]\n",
    "                    \n",
    "                    # Determine the maximum value in the pooling region\n",
    "                    max_val = np.max(x_pool)\n",
    "                    \n",
    "                    # Create a mask that is 1 at the position of the max value and 0 elsewhere\n",
    "                    mask = (x_pool == max_val)\n",
    "                    \n",
    "                    # Propagate the gradient through the max-pooling operation\n",
    "                    dx[n, c, h_start:h_end, w_start:w_end] += prev_layer_grad[n, c, h, w] * mask\n",
    "                    \n",
    "    return dx\n",
    "\n",
    "def softmax_naive(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss_naive(pred, target):\n",
    "    N = pred.shape[0]\n",
    "    clipped_pred = np.clip(pred, 1e-12, 1. - 1e-12)\n",
    "    return -np.sum(target * np.log(clipped_pred)) / N\n",
    "\n",
    "def d_cross_entropy_loss_naive(pred, target):\n",
    "    N = pred.shape[0]\n",
    "    return (pred - target) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: torch.Size([60000, 1, 28, 28])\n",
      "Train Data Type: torch.float32\n",
      "Test Data Shape: torch.Size([10000, 1, 28, 28])\n",
      "Test Data Type: torch.float32\n",
      "Iters per epoch: 937\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std of MNIST\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../../../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../../../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load all training data into system DRAM\n",
    "train_data = torch.Tensor()\n",
    "train_labels = torch.LongTensor()  # Convert to LongTensor for labels\n",
    "for batchidxev, (data, label) in enumerate(train_loader):\n",
    "    train_data = torch.cat((train_data, data), dim=0)\n",
    "    train_labels = torch.cat((train_labels, label), dim=0)\n",
    "print('Train Data Shape:', train_data.shape)\n",
    "print('Train Data Type:', train_data.dtype)\n",
    "\n",
    "# Load all test data into system DRAM\n",
    "test_data = torch.Tensor()\n",
    "test_labels = torch.LongTensor()  # Convert to LongTensor for labels\n",
    "for batchidx, (data, label) in enumerate(test_loader):\n",
    "    test_data = torch.cat((test_data, data), dim=0)\n",
    "    test_labels = torch.cat((test_labels, label), dim=0)\n",
    "print('Test Data Shape:', test_data.shape)\n",
    "print('Test Data Type:', test_data.dtype)\n",
    "\n",
    "iters_per_epoch = 60_000 // batch_size\n",
    "print('Iters per epoch:', iters_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2 [[-4.04632453e-01  1.55585324e-03 -7.22037405e-01 ... -7.72472995e-01\n",
      "   3.11575001e-01 -4.48599169e-01]\n",
      " [ 6.06264051e-02 -3.03086078e-01  3.65922641e-01 ... -1.03070662e+00\n",
      "  -1.02773165e+00  2.02246203e+00]\n",
      " [ 1.69209793e+00 -2.40898511e-01 -8.51896461e-01 ...  4.14495726e-01\n",
      "   1.12930323e+00  7.93053221e-01]\n",
      " ...\n",
      " [ 7.76044912e-01  1.37814775e+00 -1.01397074e-01 ... -7.13224821e-01\n",
      "   1.21873493e+00 -7.55031189e-01]\n",
      " [ 7.42708701e-01  3.96874734e-01  2.76460947e-01 ... -7.41453432e-01\n",
      "  -4.84037574e-01  2.45427244e-01]\n",
      " [-8.97876573e-01 -7.27971402e-01 -8.69469823e-01 ... -2.02834981e-02\n",
      "   9.00807460e-01  7.20277453e-01]]\n",
      "Model initialized\n",
      "\n",
      "\n",
      "\n",
      "Loss: 13.815510557964773\n",
      "fc2_w_grad: [[   3942.22866724    -159.35069001   16808.38610212 ...  -36131.67906102\n",
      "    32030.46091518   17255.73436866]\n",
      " [  -2106.54362975   -2482.23437908    8264.84361961 ...   -7392.39910211\n",
      "     3073.84592867  -10313.28094257]\n",
      " [  10895.28998235    5657.68982069    5133.06031656 ...  -28773.4986957\n",
      "    32321.31990561   21395.8242312 ]\n",
      " ...\n",
      " [  24094.02681412   10354.0827683    49782.99753899 ...  -95155.65764869\n",
      "    77461.63420845   29798.50215092]\n",
      " [   8283.00753271    4217.48481116   23691.98254248 ...  -35357.49692699\n",
      "    22661.48701905    6009.67334536]\n",
      " [  36865.74755975   15672.42116759   29810.01605501 ... -100245.73221625\n",
      "    94571.58687346   71953.72487131]]\n",
      "Epoch: 0 Loss: 13.815510557964773\n",
      "\n",
      "\n",
      "\n",
      "Loss: 13.815510557964773\n",
      "fc2_w_grad: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-1.12451977e+04 -6.95074211e+03 -8.04929374e+04 ...  1.45524510e+05\n",
      "  -1.36400853e+05 -1.75700413e+04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-3.00566938e+03  1.38644029e+02 -1.10008888e+04 ...  1.64004408e+04\n",
      "  -1.61306508e+04 -2.16285433e+03]\n",
      " [-1.29288303e+04 -9.19627407e+03 -3.17392511e+04 ...  6.86537129e+04\n",
      "  -6.18722227e+04 -2.73117379e+04]]\n",
      "Epoch: 1 Loss: 13.815510557964773\n",
      "\n",
      "\n",
      "\n",
      "Loss: 6.907755278982886\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 2 Loss: 6.907755278982886\n",
      "\n",
      "\n",
      "\n",
      "Loss: 20.72326583694666\n",
      "fc2_w_grad: [[      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " ...\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [   2229.28771425    1599.81304711    4864.22189005 ...  -11897.44953945\n",
      "    11077.7322549     5449.58236632]\n",
      " [ -40602.52442972   -7061.82363141  -58141.23484768 ...  126771.02064662\n",
      "  -112542.89333625  -47335.25966388]]\n",
      "Epoch: 3 Loss: 20.72326583694666\n",
      "\n",
      "\n",
      "\n",
      "Loss: 20.72326583694666\n",
      "fc2_w_grad: [[  479476.65381956   132466.33310392  1273805.64911581 ...\n",
      "  -2219796.56603763  2072375.6131285    618644.96284014]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]\n",
      " ...\n",
      " [  174616.7124687     48241.83912841   463896.94472251 ...\n",
      "   -808409.70175094   754721.66096536   225299.28983179]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]]\n",
      "Epoch: 4 Loss: 20.72326583694666\n",
      "\n",
      "\n",
      "\n",
      "Loss: 27.631021115928547\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 5 Loss: 27.631021115928547\n",
      "\n",
      "\n",
      "\n",
      "Loss: 20.72326583694666\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 6 Loss: 20.72326583694666\n",
      "\n",
      "\n",
      "\n",
      "Loss: 20.72326583694666\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 7 Loss: 20.72326583694666\n",
      "\n",
      "\n",
      "\n",
      "Loss: 24.177143476437603\n",
      "fc2_w_grad: [[      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [  25812.26652944   18169.46636623  139642.36517482 ... -267122.00252314\n",
      "   257438.98091022   54271.80850414]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " ...\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]]\n",
      "Epoch: 8 Loss: 24.177143476437603\n",
      "\n",
      "\n",
      "\n",
      "Loss: 24.177143476437603\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 9 Loss: 24.177143476437603\n",
      "\n",
      "\n",
      "\n",
      "Loss: 24.177143476437603\n",
      "fc2_w_grad: [[ -832842.77710582  -232333.74224604 -2223774.344646   ...\n",
      "   3892668.55311863 -3625312.01855657 -1095733.27200136]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]\n",
      " ...\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]\n",
      " [       0.                0.                0.         ...\n",
      "         0.                0.                0.        ]]\n",
      "Epoch: 10 Loss: 24.177143476437603\n",
      "\n",
      "\n",
      "\n",
      "Loss: 24.177143476437603\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 11 Loss: 24.177143476437603\n",
      "\n",
      "\n",
      "\n",
      "Loss: 13.815510557964775\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 12 Loss: 13.815510557964775\n",
      "\n",
      "\n",
      "\n",
      "Loss: 24.177143476437603\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 13 Loss: 24.177143476437603\n",
      "\n",
      "\n",
      "\n",
      "Loss: 27.631021115928547\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 14 Loss: 27.631021115928547\n",
      "\n",
      "\n",
      "\n",
      "Loss: 13.815510557964773\n",
      "fc2_w_grad: [[  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " ...\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [-23.55080912 -18.7467604  -55.5397043  ...  97.33431868 -55.9494242\n",
      "  -29.37962104]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]]\n",
      "Epoch: 15 Loss: 13.815510557964773\n",
      "\n",
      "\n",
      "\n",
      "Loss: 24.177143476437603\n",
      "fc2_w_grad: [[    0.             0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " [ -356.19468558  -250.72836405 -1926.98569495 ...  3686.13262189\n",
      "  -3552.51239777  -748.9202757 ]\n",
      " [    0.             0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " ...\n",
      " [    0.             0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " [    0.             0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " [    0.             0.             0.         ...     0.\n",
      "      0.             0.        ]]\n",
      "Epoch: 16 Loss: 24.177143476437603\n",
      "\n",
      "\n",
      "\n",
      "Loss: 27.631021115928547\n",
      "fc2_w_grad: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch: 17 Loss: 27.631021115928547\n",
      "\n",
      "\n",
      "\n",
      "Loss: 17.269388197455715\n",
      "fc2_w_grad: [[      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " ...\n",
      " [ -58127.27736179  -17108.88020581 -150324.39529248 ...  264395.20543091\n",
      "  -243491.42498716  -74672.98401774]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]\n",
      " [      0.               0.               0.         ...       0.\n",
      "        0.               0.        ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 161\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    160\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m10\u001b[39m)[label]\n\u001b[0;32m--> 161\u001b[0m loss, dgamma, dbeta, dw_conv1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m'\u001b[39m, loss)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Update batchnorm parameters\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# model.bn1_gamma -= learning_rate * dgamma\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# model.bn1_beta -= learning_rate * dbeta\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# gradient descent\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[140], line 120\u001b[0m, in \u001b[0;36mModel.backward\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m    114\u001b[0m flatten_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_fc1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_flatten\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# (8, 6272) -> (8, 32, 14, 14)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# print('dReshape:', dx.shape)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# print('dReshape:', dx)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# print(np.any(flatten_grad > 100) or np.any(flatten_grad < -100))\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# print(flatten_grad.shape, self.x_pool1.shape)\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[43mmax_pool2d_backward_naive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_pool1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# print('dMaxPool1:', dx.shape)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# print('dMaxPool1:', dx)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# print(np.any(dx > 100) or np.any(dx < -100))\u001b[39;00m\n\u001b[1;32m    125\u001b[0m dx \u001b[38;5;241m=\u001b[39m d_relu_naive(dx)\n",
      "Cell \u001b[0;32mIn[122], line 165\u001b[0m, in \u001b[0;36mmax_pool2d_backward_naive\u001b[0;34m(prev_layer_grad, x, pool_size, stride)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 mask \u001b[38;5;241m=\u001b[39m (x_pool \u001b[38;5;241m==\u001b[39m max_val)\n\u001b[1;32m    164\u001b[0m                 \u001b[38;5;66;03m# Propagate the gradient through the max-pooling operation\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m                 dx[n, c, h_start:h_end, w_start:w_end] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prev_layer_grad[n, c, h, w] \u001b[38;5;241m*\u001b[39m mask\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dx\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Forward pass:\n",
    "conv2d: kernel size 3x3, stride 1, padding 1, in 1 channel, out 32 channels\n",
    "batchnorm2d: out 32 channels\n",
    "relu\n",
    "maxpool2d: kernel size 2x2, stride 2\n",
    "fc1: in 32*14*14, out 128\n",
    "relu\n",
    "fc2: in 128, out 10\n",
    "log softmax (dim=1)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.conv1_w = np.random.randn(32, 1, 3, 3)\n",
    "        self.bn1_gamma = np.ones((1, 32, 1, 1), dtype=np.float32)\n",
    "        self.bn1_beta = np.zeros((1, 32, 1, 1), dtype=np.float32)\n",
    "        self.fc1_w = np.random.randn(32*14*14, 128)\n",
    "        self.fc2_w = np.random.randn(128, 10)\n",
    "        print(\"fc2\", self.fc2_w)\n",
    "\n",
    "        # init grads\n",
    "        self.conv1_w_grad = np.zeros_like(self.conv1_w)\n",
    "        self.bn1_gamma_grad = np.zeros_like(self.bn1_gamma)\n",
    "        self.bn1_beta_grad = np.zeros_like(self.bn1_beta)\n",
    "        self.fc1_w_grad = np.zeros_like(self.fc1_w)\n",
    "        self.fc2_w_grad = np.zeros_like(self.fc2_w)\n",
    "        print('Model initialized')\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_conv1 = x\n",
    "        x = conv2d_naive(x, self.conv1_w, stride=1, padding=1)\n",
    "        # print('Conv1:', x.shape)\n",
    "\n",
    "        self.x_bn1 = x\n",
    "        x, self.bn1_cache = batch_norm_naive(self.x_bn1, self.bn1_gamma, self.bn1_beta)\n",
    "        # print('BN1:', x.shape)\n",
    "        \n",
    "        self.x_relu1 = x\n",
    "        x = relu_naive(x)\n",
    "        # print('ReLU1:', x.shape)\n",
    "        \n",
    "        self.x_pool1 = x\n",
    "        x = max_pool2d_naive(x, pool_size=2, stride=2)\n",
    "        # print('MaxPool1:', x.shape)\n",
    "        \n",
    "        self.x_flatten = x\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        # print('Flatten:', x.shape)\n",
    "        \n",
    "        self.x_fc1 = x\n",
    "        x = np.dot(x, self.fc1_w)\n",
    "        # print('FC1:', x.shape)\n",
    "        \n",
    "        self.x_relu2 = x\n",
    "        x = relu_naive(x)\n",
    "        # print('ReLU2:', x.shape)\n",
    "        \n",
    "        self.x_fc2 = x\n",
    "        x = np.dot(x, self.fc2_w)\n",
    "        # print('FC2:', x.shape)\n",
    "        # check if this is filled with zeros\n",
    "        \n",
    "        # if np.all(x == 0):\n",
    "        #     print('Zero FC2')\n",
    "        # else: \n",
    "        #     print('Non-zero FC2')\n",
    "        \n",
    "        # x = softmax_naive(x)\n",
    "        # print('Softmax:', x.shape)\n",
    "        return x\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # print('y_pred:', y_pred.shape, y_pred)\n",
    "        # print('y_true:', y_true.shape, y_true)\n",
    "        loss = cross_entropy_loss_naive(y_pred, y_true)\n",
    "        print('Loss:', loss)\n",
    "\n",
    "\n",
    "        # dx -> previous layer gradient\n",
    "        dx = d_cross_entropy_loss_naive(y_pred, y_true)\n",
    "        # print('dLoss:', dx.shape) # -> (B, 10)\n",
    "        # print('dLoss:', dx)\n",
    "\n",
    "        # do this properly\n",
    "        # print('shapes', dx.T.shape, self.x_fc2.shape)\n",
    "        self.fc2_w_grad += self.x_fc2.T @ dx\n",
    "        print('fc2_w_grad:', self.fc2_w_grad)\n",
    "        # print('dFC2:', self.fc2_w_grad.shape)\n",
    "        # check if all any of the numbers are greater than 100 or less than -100\n",
    "        # if np.any(self.fc2_w_grad > 100) or np.any(self.fc2_w_grad < -100):\n",
    "        #     print('FC2 Gradient Exploded')\n",
    "        # print('dFC2:', dx)\n",
    "        # print(self.fc2_w_grad.shape, self.x_relu2.shape)  \n",
    "        relu_grad = d_relu_naive(self.x_fc2)\n",
    "        # print('dReLU2:', dx.shape)\n",
    "        # print('dReLU2:', dx)\n",
    "        # print(np.any(relu_grad > 100) or np.any(relu_grad < -100))\n",
    "\n",
    "        # print(self.x_flatten.T.shape, dx.shape)\n",
    "        # print(self.x_fc1.shape, relu_grad.shape)\n",
    "        self.fc1_w_grad = self.x_fc1.T @ relu_grad # (6272, 8) @ (8, 128) -> (6272, 128)\n",
    "\n",
    "        # print('dFC1:', dx.shape)\n",
    "        # print('dFC1:', dx)\n",
    "        # print(np.any(self.fc1_w_grad > 100) or np.any(self.fc1_w_grad < -100))\n",
    "        \n",
    "        # print(self.fc1_w_grad.shape)\n",
    "        # print(self.x_flatten.shape)\n",
    "        flatten_grad = self.x_fc1.reshape(self.x_flatten.shape) # (8, 6272) -> (8, 32, 14, 14)\n",
    "        # print('dReshape:', dx.shape)\n",
    "        # print('dReshape:', dx)\n",
    "        # print(np.any(flatten_grad > 100) or np.any(flatten_grad < -100))\n",
    "\n",
    "        # print(flatten_grad.shape, self.x_pool1.shape)\n",
    "        dx = max_pool2d_backward_naive(flatten_grad, self.x_pool1, pool_size=2, stride=2)\n",
    "        # print('dMaxPool1:', dx.shape)\n",
    "        # print('dMaxPool1:', dx)\n",
    "        # print(np.any(dx > 100) or np.any(dx < -100))\n",
    "\n",
    "        dx = d_relu_naive(dx)\n",
    "        # print('dReLU1:', dx.shape)\n",
    "        # print('dReLU1:', dx)\n",
    "        # print(np.any(dx > 100) or np.any(dx < -100))\n",
    "\n",
    "        # print('cache:', self.bn1_cache)\n",
    "        x, norm, mean, var, gamma, beta, eps = self.bn1_cache\n",
    "        dx, dgamma, dbeta = d_batch_norm_naive(norm, gamma, beta, dx, mean, var, eps)\n",
    "        # print('dBN1:', dx.shape)\n",
    "        # print('dBN1:', dx)\n",
    "        # print(np.any(dx > 100) or np.any(dx < -100))\n",
    "\n",
    "        dx, dw_conv1 = conv2d_backward_naive(dx, self.x_conv1, self.conv1_w, stride=1, padding=1)\n",
    "        # print('dConv1:', dx.shape)\n",
    "\n",
    "        # print('dConv1:', dx)\n",
    "        # print(np.any(dx > 100) or np.any(dx < -100))\n",
    "\n",
    "        return loss, dgamma, dbeta, dw_conv1\n",
    "\n",
    "model = Model()\n",
    "\n",
    "for i in range(20):\n",
    "    start_time = time.time()\n",
    " \n",
    "    data = train_data[i*batch_size:(i+1)*batch_size]\n",
    "    label = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "    data = data.numpy()\n",
    "    label = label.numpy()\n",
    "    # print('labels', label) if i == 0 else None\n",
    "    y_pred = model.forward(data)\n",
    "    # argmax for a (B, 1) output\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # print(y_pred) if i == 0 else None\n",
    "    print(\"\\n\"*2)\n",
    "    y_true = np.eye(10)[label]\n",
    "    loss, dgamma, dbeta, dw_conv1 = model.backward(y_pred, y_true)\n",
    "    print('Epoch:', i, 'Loss:', loss)\n",
    "    # Update batchnorm parameters\n",
    "    # model.bn1_gamma -= learning_rate * dgamma\n",
    "    # model.bn1_beta -= learning_rate * dbeta\n",
    "    # gradient descent\n",
    "    model.conv1_w -= learning_rate * model.conv1_w_grad\n",
    "    model.fc1_w -= learning_rate * model.fc1_w_grad\n",
    "    model.fc2_w -= learning_rate * model.fc2_w_grad\n",
    "\n",
    "    # zero grads\n",
    "    model.conv1_w_grad = np.zeros_like(model.conv1_w)\n",
    "    model.fc1_w_grad = np.zeros_like(model.fc1_w)\n",
    "    model.fc2_w_grad = np.zeros_like(model.fc2_w)\n",
    "\n",
    " \n",
    "    end_time = time.time()\n",
    "    # print('Epoch:', i, 'Time:', end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
