{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: torch.Size([60000, 1, 28, 28])\n",
      "Train Data Type: torch.float32\n",
      "Test Data Shape: torch.Size([10000, 1, 28, 28])\n",
      "Test Data Type: torch.float32\n",
      "Iters per epoch: 937\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 5e-3\n",
    "num_epochs = 5\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std of MNIST\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load all training data into system DRAM\n",
    "train_data = torch.Tensor()\n",
    "train_labels = torch.LongTensor()  # Convert to LongTensor for labels\n",
    "for batchidx, (data, label) in enumerate(train_loader):\n",
    "    train_data = torch.cat((train_data, data), dim=0)\n",
    "    train_labels = torch.cat((train_labels, label), dim=0)\n",
    "print('Train Data Shape:', train_data.shape)\n",
    "print('Train Data Type:', train_data.dtype)\n",
    "\n",
    "# Load all test data into system DRAM\n",
    "test_data = torch.Tensor()\n",
    "test_labels = torch.LongTensor()  # Convert to LongTensor for labels\n",
    "for batchidx, (data, label) in enumerate(test_loader):\n",
    "    test_data = torch.cat((test_data, data), dim=0)\n",
    "    test_labels = torch.cat((test_labels, label), dim=0)\n",
    "print('Test Data Shape:', test_data.shape)\n",
    "print('Test Data Type:', test_data.dtype)\n",
    "\n",
    "iters_per_epoch = 60_000 // batch_size\n",
    "print('Iters per epoch:', iters_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # print(\"conv\", x.shape)\n",
    "        x = self.batchnorm(x)\n",
    "        # print(\"bn\", x.shape)\n",
    "        x = self.relu(x)\n",
    "        # print(\"relu\", x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        # print(\"maxpool\", x.shape)\n",
    "        return x\n",
    "    \n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, out_channels=32):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.cnn_block1 = CNNBlock(input_channels, out_channels)\n",
    "        # self.cnn_block2 = CNNBlock(32, 64)\n",
    "        # Define further CNN blocks as needed\n",
    "        # Flatten the output from the last CNN block\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 64*7*7 is the number of features after flattening w/ 64 output channels, 7x7 spatial dimensions\n",
    "        # 32*14*14 is the number of features after flattening w/ 32 output channels, 14x14 spatial dimensions\n",
    "        self.mlp_block1 = MLPBlock(out_channels * 14 * 14, 128, num_classes)\n",
    "        # self.log_sm_out = nn.LogSoftmax(dim=1)\n",
    "        # self.sm_out = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_block1(x)\n",
    "        # x = self.cnn_block2(x)\n",
    "        # Forward through more CNN blocks if defined\n",
    "        x = self.flatten(x)\n",
    "        logits = self.mlp_block1(x)\n",
    "        # logits = self.sm_out(x)\n",
    "        return logits\n",
    "    \n",
    "model = CombinedModel(input_channels=1, num_classes=10).to('cuda')\n",
    "# model = torch.compile(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iter: 1, Loss: 2.2996788024902344\n",
      "Iteration Time: 1.2822 sec\n",
      "Epoch: 1, Iter: 100, Loss: 0.5774672031402588\n",
      "Iteration Time: 0.7610 sec\n",
      "Epoch: 1, Iter: 200, Loss: 0.4642135202884674\n",
      "Iteration Time: 0.7076 sec\n",
      "Epoch: 1, Iter: 300, Loss: 0.3546617031097412\n",
      "Iteration Time: 0.7112 sec\n",
      "Epoch: 1, Iter: 400, Loss: 0.2658010423183441\n",
      "Iteration Time: 0.7625 sec\n",
      "Epoch: 1, Iter: 500, Loss: 0.23539374768733978\n",
      "Iteration Time: 0.8154 sec\n",
      "Epoch: 1, Iter: 600, Loss: 0.2761000990867615\n",
      "Iteration Time: 0.7732 sec\n",
      "Epoch: 1, Iter: 700, Loss: 0.23922687768936157\n",
      "Iteration Time: 0.8435 sec\n",
      "Epoch: 1, Iter: 800, Loss: 0.28133729100227356\n",
      "Iteration Time: 0.8140 sec\n",
      "Epoch: 1, Iter: 900, Loss: 0.19544850289821625\n",
      "Iteration Time: 0.7532 sec\n",
      "Average Batch Accuracy: 94.84%\n",
      "Epoch: 2, Iter: 1, Loss: 0.19934824109077454\n",
      "Iteration Time: 0.9446 sec\n",
      "Epoch: 2, Iter: 100, Loss: 0.17171578109264374\n",
      "Iteration Time: 0.7701 sec\n",
      "Epoch: 2, Iter: 200, Loss: 0.21684099733829498\n",
      "Iteration Time: 0.7601 sec\n",
      "Epoch: 2, Iter: 300, Loss: 0.17679248750209808\n",
      "Iteration Time: 0.7672 sec\n",
      "Epoch: 2, Iter: 400, Loss: 0.11892186850309372\n",
      "Iteration Time: 0.7598 sec\n",
      "Epoch: 2, Iter: 500, Loss: 0.138310506939888\n",
      "Iteration Time: 0.7298 sec\n",
      "Epoch: 2, Iter: 600, Loss: 0.17731574177742004\n",
      "Iteration Time: 0.7081 sec\n",
      "Epoch: 2, Iter: 700, Loss: 0.17379729449748993\n",
      "Iteration Time: 0.7038 sec\n",
      "Epoch: 2, Iter: 800, Loss: 0.19613024592399597\n",
      "Iteration Time: 0.7155 sec\n",
      "Epoch: 2, Iter: 900, Loss: 0.14859504997730255\n",
      "Iteration Time: 0.7100 sec\n",
      "Average Batch Accuracy: 96.29%\n",
      "Epoch: 3, Iter: 1, Loss: 0.13542912900447845\n",
      "Iteration Time: 1.0123 sec\n",
      "Epoch: 3, Iter: 100, Loss: 0.12068824470043182\n",
      "Iteration Time: 0.7570 sec\n",
      "Epoch: 3, Iter: 200, Loss: 0.14078693091869354\n",
      "Iteration Time: 0.7591 sec\n",
      "Epoch: 3, Iter: 300, Loss: 0.13207708299160004\n",
      "Iteration Time: 0.7668 sec\n",
      "Epoch: 3, Iter: 400, Loss: 0.06870658695697784\n",
      "Iteration Time: 0.7334 sec\n",
      "Epoch: 3, Iter: 500, Loss: 0.09060163050889969\n",
      "Iteration Time: 0.7091 sec\n",
      "Epoch: 3, Iter: 600, Loss: 0.13428260385990143\n",
      "Iteration Time: 0.7718 sec\n",
      "Epoch: 3, Iter: 700, Loss: 0.15071111917495728\n",
      "Iteration Time: 0.8547 sec\n",
      "Epoch: 3, Iter: 800, Loss: 0.1448565274477005\n",
      "Iteration Time: 0.7198 sec\n",
      "Epoch: 3, Iter: 900, Loss: 0.12187694758176804\n",
      "Iteration Time: 0.7041 sec\n",
      "Average Batch Accuracy: 97.08%\n",
      "Epoch: 4, Iter: 1, Loss: 0.10317918658256531\n",
      "Iteration Time: 0.9084 sec\n",
      "Epoch: 4, Iter: 100, Loss: 0.09386730194091797\n",
      "Iteration Time: 0.7036 sec\n",
      "Epoch: 4, Iter: 200, Loss: 0.10520726442337036\n",
      "Iteration Time: 0.7977 sec\n",
      "Epoch: 4, Iter: 300, Loss: 0.10893300920724869\n",
      "Iteration Time: 0.7052 sec\n",
      "Epoch: 4, Iter: 400, Loss: 0.0464453250169754\n",
      "Iteration Time: 0.7181 sec\n",
      "Epoch: 4, Iter: 500, Loss: 0.06401146203279495\n",
      "Iteration Time: 0.7873 sec\n",
      "Epoch: 4, Iter: 600, Loss: 0.11102774739265442\n",
      "Iteration Time: 0.8860 sec\n",
      "Epoch: 4, Iter: 700, Loss: 0.14092174172401428\n",
      "Iteration Time: 0.7105 sec\n",
      "Epoch: 4, Iter: 800, Loss: 0.11466164886951447\n",
      "Iteration Time: 0.7336 sec\n",
      "Epoch: 4, Iter: 900, Loss: 0.10201086848974228\n",
      "Iteration Time: 0.7038 sec\n",
      "Average Batch Accuracy: 97.57%\n",
      "Epoch: 5, Iter: 1, Loss: 0.08552907407283783\n",
      "Iteration Time: 0.9556 sec\n",
      "Epoch: 5, Iter: 100, Loss: 0.07621592283248901\n",
      "Iteration Time: 0.7041 sec\n",
      "Epoch: 5, Iter: 200, Loss: 0.08454911410808563\n",
      "Iteration Time: 0.7551 sec\n",
      "Epoch: 5, Iter: 300, Loss: 0.09402839094400406\n",
      "Iteration Time: 0.8795 sec\n",
      "Epoch: 5, Iter: 400, Loss: 0.03391614928841591\n",
      "Iteration Time: 0.7386 sec\n",
      "Epoch: 5, Iter: 500, Loss: 0.048099249601364136\n",
      "Iteration Time: 0.8180 sec\n",
      "Epoch: 5, Iter: 600, Loss: 0.09575314819812775\n",
      "Iteration Time: 0.7117 sec\n",
      "Epoch: 5, Iter: 700, Loss: 0.134432852268219\n",
      "Iteration Time: 0.7033 sec\n",
      "Epoch: 5, Iter: 800, Loss: 0.09441162645816803\n",
      "Iteration Time: 0.7837 sec\n",
      "Epoch: 5, Iter: 900, Loss: 0.08822688460350037\n",
      "Iteration Time: 0.7589 sec\n",
      "Average Batch Accuracy: 97.82%\n",
      "Epoch: 6, Iter: 1, Loss: 0.07560977339744568\n",
      "Iteration Time: 0.9089 sec\n",
      "Epoch: 6, Iter: 100, Loss: 0.06294693052768707\n",
      "Iteration Time: 0.7019 sec\n",
      "Epoch: 6, Iter: 200, Loss: 0.07046416401863098\n",
      "Iteration Time: 0.7079 sec\n",
      "Epoch: 6, Iter: 300, Loss: 0.0844801664352417\n",
      "Iteration Time: 0.7186 sec\n",
      "Epoch: 6, Iter: 400, Loss: 0.026281315833330154\n",
      "Iteration Time: 0.7606 sec\n",
      "Epoch: 6, Iter: 500, Loss: 0.037779394537210464\n",
      "Iteration Time: 0.7303 sec\n",
      "Epoch: 6, Iter: 600, Loss: 0.08417430520057678\n",
      "Iteration Time: 0.7024 sec\n",
      "Epoch: 6, Iter: 700, Loss: 0.13067087531089783\n",
      "Iteration Time: 0.7026 sec\n",
      "Epoch: 6, Iter: 800, Loss: 0.08057829737663269\n",
      "Iteration Time: 0.7765 sec\n",
      "Epoch: 6, Iter: 900, Loss: 0.07759017497301102\n",
      "Iteration Time: 0.7658 sec\n",
      "Average Batch Accuracy: 97.98%\n",
      "Epoch: 7, Iter: 1, Loss: 0.06895027309656143\n",
      "Iteration Time: 0.9255 sec\n",
      "Epoch: 7, Iter: 100, Loss: 0.053415458649396896\n",
      "Iteration Time: 0.7184 sec\n",
      "Epoch: 7, Iter: 200, Loss: 0.060313880443573\n",
      "Iteration Time: 0.7164 sec\n",
      "Epoch: 7, Iter: 300, Loss: 0.07696547359228134\n",
      "Iteration Time: 0.7117 sec\n",
      "Epoch: 7, Iter: 400, Loss: 0.02129356376826763\n",
      "Iteration Time: 0.7346 sec\n",
      "Epoch: 7, Iter: 500, Loss: 0.03065095841884613\n",
      "Iteration Time: 0.7954 sec\n",
      "Epoch: 7, Iter: 600, Loss: 0.07499376684427261\n",
      "Iteration Time: 0.7050 sec\n",
      "Epoch: 7, Iter: 700, Loss: 0.12842419743537903\n",
      "Iteration Time: 0.7043 sec\n",
      "Epoch: 7, Iter: 800, Loss: 0.07035919278860092\n",
      "Iteration Time: 0.7219 sec\n",
      "Epoch: 7, Iter: 900, Loss: 0.0713370218873024\n",
      "Iteration Time: 0.7071 sec\n",
      "Average Batch Accuracy: 98.17%\n",
      "Epoch: 8, Iter: 1, Loss: 0.06355834007263184\n",
      "Iteration Time: 0.9370 sec\n",
      "Epoch: 8, Iter: 100, Loss: 0.045750562101602554\n",
      "Iteration Time: 0.7708 sec\n",
      "Epoch: 8, Iter: 200, Loss: 0.05280548706650734\n",
      "Iteration Time: 0.7563 sec\n",
      "Epoch: 8, Iter: 300, Loss: 0.07066957652568817\n",
      "Iteration Time: 0.7656 sec\n",
      "Epoch: 8, Iter: 400, Loss: 0.017832256853580475\n",
      "Iteration Time: 0.7553 sec\n",
      "Epoch: 8, Iter: 500, Loss: 0.025645921006798744\n",
      "Iteration Time: 0.7567 sec\n",
      "Epoch: 8, Iter: 600, Loss: 0.06704989075660706\n",
      "Iteration Time: 0.7603 sec\n",
      "Epoch: 8, Iter: 700, Loss: 0.12503796815872192\n",
      "Iteration Time: 0.7601 sec\n",
      "Epoch: 8, Iter: 800, Loss: 0.061514366418123245\n",
      "Iteration Time: 0.7548 sec\n",
      "Epoch: 8, Iter: 900, Loss: 0.06602557748556137\n",
      "Iteration Time: 0.7546 sec\n",
      "Average Batch Accuracy: 98.18%\n",
      "Epoch: 9, Iter: 1, Loss: 0.05922779440879822\n",
      "Iteration Time: 0.9217 sec\n",
      "Epoch: 9, Iter: 100, Loss: 0.04018688574433327\n",
      "Iteration Time: 0.7081 sec\n",
      "Epoch: 9, Iter: 200, Loss: 0.046604305505752563\n",
      "Iteration Time: 0.7048 sec\n",
      "Epoch: 9, Iter: 300, Loss: 0.06540169566869736\n",
      "Iteration Time: 0.7665 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Main\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     evaluate(model, test_data, test_labels)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m target \u001b[38;5;241m=\u001b[39m train_labels[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_block1(x)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# x = self.cnn_block2(x)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Forward through more CNN blocks if defined\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_block1(x)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# logits = self.sm_out(x)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/flatten.py:49\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the model\n",
    "def train(model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i in range(iters_per_epoch):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        data = train_data[i*batch_size:(i+1)*batch_size].to('cuda')\n",
    "        target = train_labels[i*batch_size:(i+1)*batch_size].to('cuda')\n",
    "        start = time.time()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end = time.time()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99 or i == 0:\n",
    "            print(f'Epoch: {epoch+1}, Iter: {i+1}, Loss: {loss}')\n",
    "            print(f'Iteration Time: {(end - start) * 1e3:.4f} sec')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Evaluation function to report average batch accuracy using the loaded test data\n",
    "def evaluate(model, test_data, test_labels):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_batch_accuracy = torch.tensor(0.0, device=device)\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_data) // batch_size):\n",
    "            data = test_data[i * batch_size: (i + 1) * batch_size].to(device)\n",
    "            target = test_labels[i * batch_size: (i + 1) * batch_size].to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_batch = (predicted == target).sum().item()\n",
    "            total_batch = target.size(0)\n",
    "            if total_batch != 0:  # Check to avoid division by zero\n",
    "                batch_accuracy = correct_batch / total_batch\n",
    "                total_batch_accuracy += batch_accuracy\n",
    "                num_batches += 1\n",
    "    \n",
    "    avg_batch_accuracy = total_batch_accuracy / num_batches\n",
    "    print(f'Average Batch Accuracy: {avg_batch_accuracy * 100:.2f}%')\n",
    "\n",
    "# Main\n",
    "for epoch in range(10):\n",
    "    train(model, criterion, optimizer, epoch)\n",
    "    evaluate(model, test_data, test_labels)\n",
    "    \n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
